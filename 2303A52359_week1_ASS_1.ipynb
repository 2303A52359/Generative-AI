{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNcDRFpQYZhoNH4zVNLiAbL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52359/Generative-AI/blob/main/2303A52359_week1_ASS_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.Write Python code from scratch to find error metrics of deep learning model. Actual values and deep learning model predicted values are shown in Table 1. Also compare the results with the outcomes of libraries YActual YP red 20 20.5 30 30.3 40 40.2 50 50.6 60 60.7 Tabela 1: YActual Vs. Ypred"
      ],
      "metadata": {
        "id": "cmBj9wk3O-ZE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_actual = [20, 30, 40, 50, 60]\n",
        "y_pred = [20.5, 30.3, 40.2, 50.6, 60.7]\n",
        "\n",
        "sum_absolute_error = 0\n",
        "sum_squared_error = 0\n",
        "\n",
        "for actual, pred in zip(y_actual, y_pred):\n",
        "    error = actual - pred\n",
        "    sum_absolute_error += abs(error)\n",
        "    sum_squared_error += error ** 2\n",
        "\n",
        "n = len(y_actual)\n",
        "mae = sum_absolute_error / n\n",
        "mse = sum_squared_error / n\n",
        "rmse = mse ** 0.5\n",
        "\n",
        "print(f\"MAE: {mae}\")\n",
        "print(f\"MSE: {mse}\")\n",
        "print(f\"RMSE: {rmse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHKY2GWJPA37",
        "outputId": "69b996f8-4814-421a-819c-39ab65f6aa91"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAE: 0.4600000000000016\n",
            "MSE: 0.24600000000000147\n",
            "RMSE: 0.49598387070549127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.Write python code from scratch to find evaluation metrics of deep learning model. Actual values and deep learning model predicted values are shown in Table 2. Also compare the results with outcome of libraries YActual YP red 0 0 1 1 2 0 0 0 1 0 2 0 0 1 1 2 2 1 0 2 1 0 2 2 0 2 1 2 2 2 Tabela 2: YActual Vs. YPred"
      ],
      "metadata": {
        "id": "yAW0pIsMPNdr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_actual, y_pred):\n",
        "    metrics = {}\n",
        "\n",
        "    # Calculate confusion matrix components\n",
        "    tp = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 1)  # True Positives\n",
        "    tn = sum(1 for a, p in zip(y_actual, y_pred) if a == p and p == 0)  # True Negatives\n",
        "    fp = sum(1 for a, p in zip(y_actual, y_pred) if a != p and p == 1)  # False Positives\n",
        "    fn = sum(1 for a, p in zip(y_actual, y_pred) if a != p and p == 0)  # False Negatives\n",
        "\n",
        "    # Accuracy\n",
        "    metrics['accuracy'] = (tp + tn) / len(y_actual)\n",
        "\n",
        "    # Precision\n",
        "    metrics['precision'] = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "\n",
        "    # Recall\n",
        "    metrics['recall'] = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "    # F1 Score\n",
        "    precision = metrics['precision']\n",
        "    recall = metrics['recall']\n",
        "    metrics['f1_score'] = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Actual and predicted values from Table 2\n",
        "y_actual = [0, 0, 1, 1, 2, 0, 0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2]\n",
        "y_pred = [0, 0, 1, 0, 2, 0, 0, 1, 1, 2, 2, 1, 0, 2, 1, 0, 2, 2, 0, 2, 1, 2, 2, 2, 0, 2, 1, 2, 2, 2]\n",
        "\n",
        "# Calculate metrics from scratch\n",
        "scratch_metrics = calculate_metrics(y_actual, y_pred)\n",
        "print(\"Metrics calculated from scratch:\")\n",
        "for metric, value in scratch_metrics.items():\n",
        "    print(f\"{metric}: {value:.2f}\")\n",
        "\n",
        "# Comparison using sklearn\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "accuracy = accuracy_score(y_actual, y_pred)\n",
        "precision = precision_score(y_actual, y_pred, average='macro')  # Macro average for multi-class\n",
        "recall = recall_score(y_actual, y_pred, average='macro')\n",
        "f1 = f1_score(y_actual, y_pred, average='macro')\n",
        "\n",
        "print(\"\\nMetrics calculated using sklearn:\")\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall: {recall:.2f}\")\n",
        "print(f\"F1 Score: {f1:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwiCM42tPOq8",
        "outputId": "163c0d61-c758-4ee1-d6a6-7fcba79a8fa3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metrics calculated from scratch:\n",
            "accuracy: 0.40\n",
            "precision: 0.71\n",
            "recall: 0.71\n",
            "f1_score: 0.71\n",
            "\n",
            "Metrics calculated using sklearn:\n",
            "Accuracy: 0.73\n",
            "Precision: 0.74\n",
            "Recall: 0.72\n",
            "F1 Score: 0.72\n"
          ]
        }
      ]
    }
  ]
}